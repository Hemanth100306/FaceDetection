{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somuk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\somuk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 478ms/step - accuracy: 0.2456 - loss: 1.8220 - val_accuracy: 0.3675 - val_loss: 1.6311\n",
      "Epoch 2/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 135ms/step - accuracy: 0.3798 - loss: 1.5851 - val_accuracy: 0.4480 - val_loss: 1.4649\n",
      "Epoch 3/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.4425 - loss: 1.4391 - val_accuracy: 0.4785 - val_loss: 1.3614\n",
      "Epoch 4/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.4801 - loss: 1.3548 - val_accuracy: 0.4970 - val_loss: 1.3107\n",
      "Epoch 5/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.4958 - loss: 1.3030 - val_accuracy: 0.5206 - val_loss: 1.2599\n",
      "Epoch 6/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.5299 - loss: 1.2351 - val_accuracy: 0.5358 - val_loss: 1.2302\n",
      "Epoch 7/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.5428 - loss: 1.2129 - val_accuracy: 0.5233 - val_loss: 1.2500\n",
      "Epoch 8/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.5569 - loss: 1.1642 - val_accuracy: 0.5504 - val_loss: 1.1892\n",
      "Epoch 9/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.5665 - loss: 1.1376 - val_accuracy: 0.5478 - val_loss: 1.1874\n",
      "Epoch 10/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.5810 - loss: 1.0954 - val_accuracy: 0.5523 - val_loss: 1.1906\n",
      "Epoch 11/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.5964 - loss: 1.0662 - val_accuracy: 0.5489 - val_loss: 1.1948\n",
      "Epoch 12/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.6072 - loss: 1.0212 - val_accuracy: 0.5551 - val_loss: 1.1747\n",
      "Epoch 13/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.6235 - loss: 0.9901 - val_accuracy: 0.5607 - val_loss: 1.1890\n",
      "Epoch 14/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.6332 - loss: 0.9548 - val_accuracy: 0.5532 - val_loss: 1.2048\n",
      "Epoch 15/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.6479 - loss: 0.9234 - val_accuracy: 0.5534 - val_loss: 1.2272\n",
      "Epoch 16/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.6601 - loss: 0.8953 - val_accuracy: 0.5558 - val_loss: 1.2299\n",
      "Epoch 17/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.6694 - loss: 0.8582 - val_accuracy: 0.5576 - val_loss: 1.2521\n",
      "Epoch 18/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.6787 - loss: 0.8333 - val_accuracy: 0.5543 - val_loss: 1.2540\n",
      "Epoch 19/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.6870 - loss: 0.8120 - val_accuracy: 0.5611 - val_loss: 1.2978\n",
      "Epoch 20/20\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.6944 - loss: 0.7907 - val_accuracy: 0.5532 - val_loss: 1.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_size = 48\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Spam\\archive\\train\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Spam\\archive\\train\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 1)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save model\n",
    "model.save('emotion_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Predicted Emotion: Angry\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\"emotion_detection_model.h5\")\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
    "    return img\n",
    "\n",
    "# Define emotion labels (ensure it matches your training classes)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = r\"C:\\Spam\\archive\\train\\disgust\\Training_69367360.jpg\" # Replace with actual image path\n",
    "processed_img = preprocess_image(img_path)\n",
    "\n",
    "# Predict emotion\n",
    "prediction = model.predict(processed_img)\n",
    "predicted_class = np.argmax(prediction)  # Get class with highest probability\n",
    "print(\"Predicted Emotion:\", emotion_labels[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somuk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 274ms/step - accuracy: 0.2646 - loss: 1.8187 - val_accuracy: 0.3235 - val_loss: 1.6910\n",
      "Epoch 2/5\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 426ms/step - accuracy: 0.3150 - loss: 1.7102 - val_accuracy: 0.3304 - val_loss: 1.6747\n",
      "Epoch 3/5\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 381ms/step - accuracy: 0.3307 - loss: 1.6823 - val_accuracy: 0.3379 - val_loss: 1.6650\n",
      "Epoch 4/5\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 288ms/step - accuracy: 0.3304 - loss: 1.6768 - val_accuracy: 0.3409 - val_loss: 1.6552\n",
      "Epoch 5/5\n",
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 342ms/step - accuracy: 0.3363 - loss: 1.6679 - val_accuracy: 0.3414 - val_loss: 1.6533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Set image size and batch size\n",
    "img_size = 48\n",
    "batch_size = 32\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    validation_split=0.2,  \n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Spam\\archive\\train\",  \n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',  # Convert grayscale to RGB\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Spam\\archive\\train\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',  # Convert grayscale to RGB\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load Pretrained VGG16 Model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "\n",
    "# Freeze the VGG16 layers (keep learned features)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create new model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=5)\n",
    "\n",
    "# Save Model\n",
    "model.save(\"improved_emotion_model.h5\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "Predicted Emotion: Happy\n",
      "Prediction Probabilities: [[0.17458038 0.01921446 0.11295405 0.3916243  0.1125332  0.11869187\n",
      "  0.07040177]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load Trained Model\n",
    "model = tf.keras.models.load_model(\"improved_emotion_model.h5\")\n",
    "\n",
    "# Define emotion labels (ensure it matches training classes)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Function to preprocess new images\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)  # Read image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (48, 48))  # Resize\n",
    "    img = img / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Test Image Path (Change this)\n",
    "img_path = r\"C:\\Spam\\archive\\test\\angry\\PrivateTest_2775163.jpg\"\n",
    "\n",
    "# Predict Emotion\n",
    "processed_img = preprocess_image(img_path)\n",
    "prediction = model.predict(processed_img)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted Emotion:\", emotion_labels[predicted_class])\n",
    "print(\"Prediction Probabilities:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
